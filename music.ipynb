{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Music generation using Artificial Intelligence\n",
        "\n",
        "## Problem: how to generate music automatically using AI ?\n",
        "\n",
        "This practical exercise is designed to teach how to automatically generate music using a Recurrent Neural Network (RNN).\n",
        "\n",
        "We do not necessarily need to be music experts to create music. Even someone without expertise in music can generate high-quality compositions using an RNN. Everyone enjoys listening to interesting music, and if there is a way to automatically generate music, especially music of acceptable quality, it represents a significant breakthrough in the music industry.\n",
        "\n",
        "Task:\n",
        "Our task is to take existing music data, then train a model using this data. The model must learn the patterns of music, so that after training, it can generate completely new compositions. It cannot simply copy and paste from the training data. Instead, it needs to understand the structure of music in order to create new melodies. While we do not expect our model to generate professional-level music, we aim for it to produce coherent and melodious compositions that are pleasant to listen to.\n",
        "\n",
        "Now, what exactly is music? Simply put, music is nothing more than a sequence of musical events (notes). The model will automatically generate a new sequence of notes, forming a complete composition. In this practical exercise, we focus only on single-instrument music, as this serves as a demonstration model. However, the model can be further improved to include multi-instrument compositions if necessary.\n",
        "\n",
        "## Data files (music files in ABC format) are available at:\n",
        "1. http://abc.sourceforge.net/NMD/\n",
        "\n",
        "### For training this script uses sample data file:\n",
        "* Jigs (340 melodies)\n",
        "\n",
        "# Program script\n",
        "Import required libraries"
      ],
      "metadata": {
        "id": "0dcn-uKjEGNZ"
      },
      "id": "0dcn-uKjEGNZ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c4c55e19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c55e19",
        "outputId": "687307d7-0590-44dd-8bd3-c7fb7e302fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039fe350",
      "metadata": {
        "id": "039fe350"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Select data folder and data file for training\n"
      ],
      "metadata": {
        "id": "Kn-5vV3aEXB_"
      },
      "id": "Kn-5vV3aEXB_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee52bd68",
      "metadata": {
        "id": "ee52bd68"
      },
      "outputs": [],
      "source": [
        "data_directory = \"/content/drive/MyDrive/Data\"  # data folder\n",
        "data_file = \"slip.txt\"   # music file\n",
        "charIndex_json = \"char_to_index.json\"\n",
        "model_weights_directory = 'Data/Model_Weights/' # Folder for trained model weights\n",
        "BATCH_SIZE = 18  # model hyperparameter\n",
        "SEQ_LENGTH = 136  # model hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation procedure"
      ],
      "metadata": {
        "id": "4l9eKnmLEeln"
      },
      "id": "4l9eKnmLEeln"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c65b428",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3c65b428"
      },
      "outputs": [],
      "source": [
        "def read_batches(all_chars, unique_chars):\n",
        "    length = all_chars.shape[0]\n",
        "    batch_chars = int(length / BATCH_SIZE) #155222/16 = 9701\n",
        "\n",
        "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 9637, 64)  #it denotes number of batches. It runs everytime when\n",
        "        #new batch is created. We have a total of 151 batches.\n",
        "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
        "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 87)\n",
        "        for batch_index in range(0, 16):  #it denotes each row in a batch.\n",
        "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means\n",
        "                #each time-step character in a sequence.\n",
        "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
        "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
        "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
        "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
        "        yield X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the architecture of the neural network model"
      ],
      "metadata": {
        "id": "Gq4EDY0AEiHO"
      },
      "id": "Gq4EDY0AEiHO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634e421d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "634e421d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85975e2c",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "85975e2c"
      },
      "outputs": [],
      "source": [
        "def build_model(batch_size, seq_length, unique_chars):\n",
        "    model = Sequential([\n",
        "        # Define an explicit Input layer to enforce batch size\n",
        "        Input(batch_shape=(batch_size, seq_length)),\n",
        "\n",
        "        # Embedding layer (batch size is NOT set here)\n",
        "        Embedding(input_dim=unique_chars, output_dim=512, mask_zero=True),\n",
        "\n",
        "        # LSTM layers (batch size is now inherited from Input layer)\n",
        "        LSTM(256, return_sequences=True, stateful=True),\n",
        "        LSTM(128, return_sequences=True, stateful=True),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(unique_chars, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create and train the model.<br>\n",
        "The model weights are saved in H5 weight files after 10 ... 80 (every 10) epochs."
      ],
      "metadata": {
        "id": "7tp8iYb5ElX_"
      },
      "id": "7tp8iYb5ElX_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c60ae3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c9c60ae3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315f39ef",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "315f39ef"
      },
      "outputs": [],
      "source": [
        "def training_model(data, epochs=80):\n",
        "    # Mapping characters to indices\n",
        "    char_to_index = {ch: i for i, ch in enumerate(sorted(set(data)))}\n",
        "    print(f\"Number of unique characters in the dataset: {len(char_to_index)}\")  # 87\n",
        "\n",
        "    # Ensure the directory exists before saving the character index JSON\n",
        "    if not os.path.exists(data_directory):\n",
        "        os.makedirs(data_directory)\n",
        "\n",
        "    with open(os.path.join(data_directory, charIndex_json), \"w\") as f:\n",
        "        json.dump(char_to_index, f)\n",
        "\n",
        "    # Create reverse mapping (index to character)\n",
        "    index_to_char = {i: ch for ch, i in char_to_index.items()}\n",
        "    unique_chars = len(char_to_index)\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = build_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)  # Fix: Correct function name `build_model`\n",
        "    model.summary()\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Convert characters to numeric indices\n",
        "    all_characters = np.array([char_to_index[c] for c in data], dtype=np.int32)\n",
        "    print(f\"Total number of characters: {all_characters.shape[0]}\")  # Expected output: 155222\n",
        "\n",
        "    epoch_number, loss, accuracy = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
        "        epoch_number.append(epoch + 1)\n",
        "\n",
        "        # Iterate over batches\n",
        "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
        "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y)  # Train on each batch\n",
        "            print(f\"Batch: {i+1}, Loss: {final_epoch_loss:.4f}, Accuracy: {final_epoch_accuracy:.4f}\")\n",
        "\n",
        "        # Store loss and accuracy for this epoch\n",
        "        loss.append(final_epoch_loss)\n",
        "        accuracy.append(final_epoch_accuracy)\n",
        "\n",
        "        # Save weights every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            if not os.path.exists(model_weights_directory):\n",
        "                os.makedirs(model_weights_directory)\n",
        "            weights_path = os.path.join(model_weights_directory, f\"Weights_{epoch+1}.weights.h5\")\n",
        "            model.save_weights(weights_path)\n",
        "            print(f\"âœ… Saved weights at epoch {epoch+1} to {weights_path}\")\n",
        "\n",
        "    print(\"ðŸŽ‰ Training completed!\")\n",
        "\n",
        "    return model, epoch_number, loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training\n",
        "Training can require a lot of time.\n",
        "An already trained model can be used instead, if you have the Weight files from the previous session. <br>"
      ],
      "metadata": {
        "id": "EToWnH6rEp7W"
      },
      "id": "EToWnH6rEp7W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da9b1ff",
      "metadata": {
        "id": "2da9b1ff"
      },
      "outputs": [],
      "source": [
        "file = open(os.path.join(data_directory, data_file), mode = 'r');\n",
        "data = file.read()\n",
        "file.close()\n",
        "if __name__ == \"__main__\":\n",
        "    training_model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the model\n",
        "Create the music generation model."
      ],
      "metadata": {
        "id": "dg_22y8DEs8W"
      },
      "id": "dg_22y8DEs8W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12522e88",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "12522e88"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26636073",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "26636073"
      },
      "outputs": [],
      "source": [
        "def make_model(batch_size, unique_chars):\n",
        "    model = Sequential([\n",
        "        # Input Layer (Ensures Fixed Batch Size for Stateful LSTMs)\n",
        "        Input(batch_shape=(batch_size, 1)),\n",
        "\n",
        "        # Embedding Layer (mask_zero=True helps with padded sequences)\n",
        "        Embedding(input_dim=unique_chars, output_dim=512, mask_zero=True),\n",
        "\n",
        "        # First LSTM Layer (Stateful, without batch_input_shape)\n",
        "        LSTM(256, return_sequences=True, stateful=True),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Second LSTM Layer (Stateful, not returning sequences)\n",
        "        LSTM(128, stateful=True),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Dense Output Layer (with activation)\n",
        "        Dense(unique_chars, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read the weights of the trained model. <br>\n",
        "Run the model and return the results."
      ],
      "metadata": {
        "id": "YKfiSbsUExgY"
      },
      "id": "YKfiSbsUExgY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc85ea5",
      "metadata": {
        "id": "4fc85ea5"
      },
      "outputs": [],
      "source": [
        "def generate_sequence(epoch_num, initial_index, seq_length):\n",
        "    with open(os.path.join(data_directory, charIndex_json)) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    unique_chars = len(index_to_char)\n",
        "\n",
        "    model = make_model(1,unique_chars)\n",
        "    model.load_weights(model_weights_directory + \"Weights_{}.weights.h5\".format(epoch_num))\n",
        "\n",
        "\n",
        "    sequence_index = [initial_index]\n",
        "\n",
        "    for _ in range(seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "\n",
        "        predicted_probs = model.predict_on_batch(batch).ravel()\n",
        "        sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)\n",
        "\n",
        "        sequence_index.append(sample[0])\n",
        "\n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "\n",
        "    cnt = 0\n",
        "    for i in seq:\n",
        "        cnt += 1\n",
        "        if i == \"\\n\":\n",
        "            break\n",
        "    seq1 = seq[cnt:]\n",
        "    #above code is for ignoring the starting string of a generated sequence. This is because we are passing any arbitrary\n",
        "    #character to the model for generating music. Now, the model start generating sequence from that character itself which we\n",
        "    #have passed, so first few characters before \"\\n\" contains meaningless word. Model start generating the music rhythm from\n",
        "    #next line onwards. The correct sequence it start generating from next line onwards which we are considering.\n",
        "\n",
        "    cnt = 0\n",
        "    for i in seq1:\n",
        "        cnt += 1\n",
        "        if i == \"\\n\" and seq1[cnt] == \"\\n\":\n",
        "            break\n",
        "    seq2 = seq1[:cnt]\n",
        "    #Now our data contains three newline characters after every tune. So, the model has leart that too. So, above code is used for\n",
        "    #ignoring all the characters that model has generated after three new line characters. So, here we are considering only one\n",
        "    #tune of music at a time and finally we are returning it..\n",
        "\n",
        "    return seq2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Music generation"
      ],
      "metadata": {
        "id": "oou5GJUmEz4t"
      },
      "id": "oou5GJUmEz4t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dff8916",
      "metadata": {
        "id": "9dff8916"
      },
      "outputs": [],
      "source": [
        "ep = int(input(\"1. Which model to use (10, 20, 30, ..., 80)? Smaller number may mean less quality: \"))\n",
        "ar = int(input(\"\\n2. Enter the generation seed (any number from 0 to 86): \"))\n",
        "ln = int(input(\"\\n3. Enter the length of sequence (from 300 to 600). Smaller number means shorter musical sequence: \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77dd531",
      "metadata": {
        "id": "b77dd531"
      },
      "source": [
        "ep = 80\n",
        "ar = 11\n",
        "ln = 600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e20da2",
      "metadata": {
        "id": "c6e20da2"
      },
      "outputs": [],
      "source": [
        "music = generate_sequence(ep, ar, ln)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3658849b",
      "metadata": {
        "id": "3658849b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMusic sequence generated by AI: \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a60181d",
      "metadata": {
        "id": "9a60181d"
      },
      "outputs": [],
      "source": [
        "print(music)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listen to the generated music!\n",
        "Copy the created sequence to https://editor.drawthedots.com/"
      ],
      "metadata": {
        "id": "U6vnOuiKE5NW"
      },
      "id": "U6vnOuiKE5NW"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
